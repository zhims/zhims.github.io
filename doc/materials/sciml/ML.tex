%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   Author:Yao Zhang  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Email: jaafar_zhang@163.com %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\usepackage{babel}
\usepackage[utf8]{inputenc} 
\usepackage[table]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage[left=2.50cm, right=1.50cm, top=2.0cm, bottom=2.50cm]{geometry}
\usepackage{xcolor,url}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd,multirow,booktabs,fullpage,calc,multicol}
\usepackage{lastpage,enumitem,fancyhdr,mathrsfs,wrapfig,setspace,cancel,amsmath,empheq,framed}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{subfig,graphicx,framed}
\usepackage{ctex}
\usepackage{txfonts}
\usepackage{bbm}
\usepackage{chngcntr}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=green,citecolor=red,urlcolor=blue]{hyperref}
\usepackage{titlesec}
\usepackage{romannum}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{thm}{Theorem}[section]
\newtheorem{defi}{Definition}[subsection]
\newtheorem{exercise}{Exercise}[subsection]
\newtheorem{note}{Note}[subsection]
\newtheorem{notation}{Notation}
\newtheorem{lemma}{Lemma}[subsection]
\newtheorem{proposition}{Proposition}[subsection]
\newtheorem{example}{Example}[subsection]
\newtheorem{problem}{Problem}[section]
\newtheorem{homework}{Homework}[section]
\newtheorem{summary}{Summary}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{rmk}{Remark}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{ {img/EoM/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\renewcommand{\cite}[1]{[#1]}
\makeatletter
\@addtoreset{equation}{section}
\makeatother
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\contentsname}{\centering \small \color{blue} Contents}
%\counterwithin{figure}{section}
\renewcommand{\figurename}{\textbf{Fig.}}
%\renewcommand{\refname}{\textbf{\kaishu 参考文献}}
\renewcommand{\refname}{\textbf{Bibliography}}
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\def\beginrefs{\begin{list}%
		{[\arabic{equation}]}{\usecounter{equation}
			\setlength{\leftmargin}{0.8truecm}\setlength{\labelsep}{0.4truecm}%
			\setlength{\labelwidth}{1.6truecm}}}
	\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[!htb]
%	\centering
%	\subfloat[$A \cap B$]{%
%		\includegraphics[width=0.3\linewidth,height=0.2\linewidth]{img001.jpg}}
%	\label{img001}\qquad \qquad %\hfill
%	\subfloat[${A_1} \cap {A_2} \cap {A_3}$]{%
%		\includegraphics[width=0.3\linewidth,height=0.2\linewidth]{img002.jpg}}
%	\label{img002}
	%\caption{ Examples.}
%\end{figure}
%\begin{figure}[!htb]
%	\centering
%	\includegraphics[width=0.4\linewidth,height=0.3\linewidth]{img005.jpg}
%	\label{img005}
	%\caption{ illustration for $ 3 $}
%\end{figure}
%\={a}1 \'{a}2\v{a}3\.{a}4

\usepackage{datetime}
\renewcommand{\today}{\shortmonthname[\the\month] \the \day,  \the\year}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	\kaishu 
	%\thispagestyle{empty}
	\pagenumbering{arabic}
	\setcounter{section}{0}
	\begin{center}
		{\LARGE  \href{https://people.eecs.berkeley.edu/~jrs/189s24/}{Introduction to Machine Learning}}
		
		\vspace{-0.25cm}
		
		{\large \href{https://people.eecs.berkeley.edu/~jrs/}{Jonathan Shewchuk}}
	\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\newpage 
%%\thispagestyle{empty}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents	
%{\pagestyle{empty}\mbox{}\newpage\pagestyle{empty}}
%\newpage 
%{\pagestyle{empty}\mbox{}\newpage\pagestyle{empty}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\newpage 
\setcounter{page}{1}

\vspace{-1cm}

\begin{enumerate}
	\item  \href{https://mp.weixin.qq.com/s/zLZz3cpvmCjDR2PdJ2qb_g}{Introduction. Classification. Training, validation, and testing. Overfitting and underfitting.} %1
	\item  \href{https://mp.weixin.qq.com/s/SGqzoJ22wzwbLHpHSFGSFg}{Linear classifiers. Decision functions and decision boundaries. The centroid method. Perceptrons.} %2
	\item  \href{https://mp.weixin.qq.com/s/aFAN-kjJ2YDEHtxrZxP_3g}{Gradient descent, stochastic gradient descent, and the perceptron learning algorithm. Feature space versus weight space. The maximum margin classifier, aka hard-margin support vector machine (SVM).} %3
	\item  \href{https://mp.weixin.qq.com/s/a7besi4VC38vRdHASzDtMQ}{The support vector classifier, aka soft-margin support vector machine (SVM). Features and nonlinear decision boundaries.} %4
	\item  \href{https://mp.weixin.qq.com/s/EVWSECLifCrTRuo2PwJpxg}{Machine learning abstractions: application/data, model, optimization problem, optimization algorithm. Common types of optimization problems: unconstrained, linear programs, quadratic programs. The influence of the step size on gradient descent.} %5
	\item  \href{https://mp.weixin.qq.com/s/RWuGUKQGb7XOVRhLGPobLA}{Decision theory, also known as risk minimization: the Bayes decision rule and the Bayes risk. Generative and discriminative models.} %6
	\item  \href{https://mp.weixin.qq.com/s/zi6zvmfnoiLEw0qBEr_Omg}{Gaussian discriminant analysis, including quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA). Maximum likelihood estimation (MLE) of the parameters of a statistical model. Fitting an isotropic Gaussian distribution to sample points.} %7
	\item  \href{https://mp.weixin.qq.com/s/itd9chjSjtMfRzDuc_uKvA}{Eigenvectors, eigenvalues, and the eigendecomposition of a symmetric real matrix. The quadratic form and ellipsoidal isosurfaces as an intuitive way of understanding symmetric matrices. Application to anisotropic multivariate normal distributions. The covariance of random variables.} %8
	\item  \href{https://mp.weixin.qq.com/s/SSjJQtt7FR7h0JBINyp1zA}{MLE, QDA, and LDA revisited for anisotropic Gaussians.} %9
	\item  \href{https://mp.weixin.qq.com/s/A1pVIIu3EWW8Spr6LYO_Jw}{Regression: fitting curves to data. The 3-choice menu of regression function + loss function + cost function. Least-squares linear regression as quadratic minimization. The design matrix, the normal equations, the pseudoinverse, and the hat matrix (projection matrix). Logistic regression; how to compute it with gradient descent or stochastic gradient descent.} %10
	\item  \href{https://mp.weixin.qq.com/s/TkOHwo93UclwmCnvE5N8HA}{Newton's method and its application to logistic regression. LDA vs. logistic regression: advantages and disadvantages. ROC curves. Weighted least-squares regression. Least-squares polynomial regression.} %11
	\item  \href{https://mp.weixin.qq.com/s/PYc0w1aeE0wFCVV9qNDPTw}{Statistical justifications for regression. The empirical distribution and empirical risk. How the principle of maximum likelihood motivates the cost functions for least-squares linear regression and logistic regression. The bias-variance decomposition; its relationship to underfitting and overfitting; its application to least-squares linear regression.} %12
	\item  \href{https://mp.weixin.qq.com/s/z_cQPMdVRTV_uZx4XBBdQw}{Ridge regression: penalized least-squares regression for reduced overfitting. How the principle of maximum a posteriori (MAP) motivates the penalty term (aka Tikhonov regularization). Subset selection. Lasso: penalized least-squares regression for reduced overfitting and subset selection.} %13
	\item  \href{https://mp.weixin.qq.com/s/EaDBBZD-1iMa7A1OxoAiFg}{Decision trees; algorithms for building them. Entropy and information gain.} %14
	\item  \href{https://mp.weixin.qq.com/s/B1dW7JklqU-R-Pld4Unc5Q}{More decision trees: decision tree regression; stopping early; pruning; multivariate splits. Ensemble learning, bagging (bootstrap aggregating), and random forests.} %15
	\item  \href{https://mp.weixin.qq.com/s/mpPLcwThOWUvIr6y_9KaOw}{More decision trees: decision tree regression; stopping early; pruning; multivariate splits. Ensemble learning, bagging (bootstrap aggregating), and random forests.} %16
	\item  \href{https://mp.weixin.qq.com/s/lm9wx-ttHjbzrHwfs5KTVQ}{Neural networks. Gradient descent and the backpropagation algorithm.} %17
	\item  \href{https://mp.weixin.qq.com/s/zhAPyEbmTizX6mFx__b1RA}{The vanishing gradient problem. Rectified linear units (ReLUs). Backpropagation with softmax outputs and cross-entropy loss. Neuron biology: axons, dendrites, synapses, action potentials. Differences between traditional computational models and neuronal computational models.} %18
	\item  \href{https://mp.weixin.qq.com/s/eawa_vyVZhCad59SNautRQ}{Heuristics for faster training. Heuristics for avoiding bad local minima. Heuristics to avoid overfitting. Convolutional neural networks. Neurology of retinal ganglion cells in the eye and simple and complex cells in the V1 visual cortex.} %19
	\item  \href{https://mp.weixin.qq.com/s/xc-ND7VF5KPcOJs0vlph9g}{Unsupervised learning. Principal components analysis (PCA). Derivations from maximum likelihood estimation, maximizing the variance, and minimizing the sum of squared projection errors. Eigenfaces for face recognition.} %20
	\item  \href{https://mp.weixin.qq.com/s/NylGFLS2hpNb4zr748I4FQ}{The singular value decomposition (SVD) and its application to PCA. Clustering: k-means clustering aka Lloyd's algorithm; k-medoids clustering; hierarchical clustering; greedy agglomerative clustering. Dendrograms. } %21
	\item  \href{https://mp.weixin.qq.com/s/zx9pddBElzCNFuR45IG9DA}{The geometry of high-dimensional spaces. Random projection. The pseudoinverse and its relationship to the singular value decomposition.} %22
	\item  \href{https://mp.weixin.qq.com/s/PzYpBKg1uC1HJ0fg4QOawA}{ Learning theory. Range spaces (aka set systems) and dichotomies. The shatter function and the Vapnik–Chervonenkis dimension.} %23
	\item  \href{https://mp.weixin.qq.com/s/ZV-BBhSOEkqZ1YiWtEqezg}{AdaBoost, a boosting method for ensemble learning. Nearest neighbor classification and its relationship to the Bayes risk.} %24
	\item  \href{https://mp.weixin.qq.com/s/dZRt8hDcFl_0-X60xCdg4w}{The exhaustive algorithm for k-nearest neighbor queries. Speeding up nearest neighbor queries. Voronoi diagrams and point location. k-d trees. Application of nearest neighbor search to the problem of geolocalization: given a query photograph, determine where in the world it was taken.} %25
	\item \href{https://pan.baidu.com/s/1h2j9Y4cql1mcMSs5X8MIqw}{Materials} 
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{ieeetr} % number
%%\bibliographystyle{unsrtnat} % author year
%\bibliography{HeBib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{flushright}
	\tiny \today 
\end{flushright}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
              